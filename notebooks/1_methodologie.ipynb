{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Effet des differents Algorithmes d'optimisation sur l'erreur de Généralisation:cas des données tabulaires**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**le choix de la fonction cout à optimiser etant une variante du domaine d'etude et du type de projet; nous nous devons d'ore et dejà de postuler pour un domaine d'etude et du choix de la fonction de cout la plus recommandée pour ce domaine.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La base de données **load_breast_cancer** est une collection de données sur des cas de cancer du sein, intégrée dans la bibliothèque **scikit-learn** en Python. Voici une description concise de cette base :\n",
    "\n",
    "### Contexte\n",
    "- **Origine** : La base de données provient de l'Université de Californie à Irvine (UCI Machine Learning Repository).\n",
    "- **Année de publication** : Les données ont été collectées à la fin des années 1980.\n",
    "\n",
    "### Utilité\n",
    "- **Classification** : Utilisée principalement pour des tâches de classification binaire, où l'objectif est de prédire si une tumeur est maligne (cancer) ou bénigne (non-cancer).\n",
    "- **Modélisation** : Permet de développer et d'évaluer des modèles d'apprentissage automatique pour la détection du cancer.\n",
    "\n",
    "### Domaine\n",
    "- **Santé** : Appartient au domaine médical, plus précisément à l'oncologie.\n",
    "- **Data Science** : Utilisée dans la recherche en data science pour développer des algorithmes de prédiction.\n",
    "\n",
    "### Caractéristiques de la Base\n",
    "- **Échantillons** : Contient 569 échantillons.\n",
    "- **Caractéristiques** : 30 caractéristiques numériques décrivant les cellules tumorales, telles que :\n",
    "  - Rayon, texture, périmètre, aire, etc.\n",
    "  - Mesures dérivées (moyenne, écart-type, etc.)\n",
    "- **Cibles** : Classifications binaires (0 pour bénin, 1 pour malin).\n",
    "\n",
    "### Importance\n",
    "- **Accessibilité** : Facilement accessible via des bibliothèques de machine learning comme scikit-learn, ce qui en fait un excellent point de départ pour les novices.\n",
    "- **Benchmarking** : Sert de référence pour tester et comparer différents algorithmes d'apprentissage automatique.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **CHOIX DE LA FONCTION DE COÛT SELON LA LITTÉRATURE SELON LE TYPE DE PROBLÈME**\n",
    "\n",
    "### 1. Fonctions de Coût pour la Régression\n",
    "\n",
    "- **Erreur Quadratique Moyenne (MSE)** :\n",
    "  $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2 $$\n",
    "  - Utilisée pour mesurer la moyenne des carrés des erreurs.\n",
    "\n",
    "- **Erreur Absolue Moyenne (MAE)** :\n",
    "  $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} |y_i - \\hat{y}_i| $$\n",
    "  - Mesure la moyenne des erreurs absolues.\n",
    "\n",
    "- **Erreur Quadratique Moyenne Racine (RMSE)** :\n",
    "  $$ J(\\theta) = \\sqrt{\\frac{1}{m} \\sum_{i=1}^{m} (y_i - \\hat{y}_i)^2} $$\n",
    "  - Prend la racine carrée de la MSE pour ramener l'échelle à celle des valeurs d'entrée.\n",
    "\n",
    "- **Erreur Logarithmique (Log-Cosh)** :\n",
    "  $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\log(\\cosh(\\hat{y}_i - y_i)) $$\n",
    "  - Moins sensible aux valeurs aberrantes que la MSE.\n",
    "\n",
    "### 2. Fonctions de Coût pour la Classification\n",
    "\n",
    "- **Entropie Croisée (Binary Cross-Entropy)** pour la classification binaire :\n",
    "  $$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right] $$\n",
    "\n",
    "- **Entropie Croisée (Categorical Cross-Entropy)** pour la classification multiclasses :\n",
    "  $$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{C} y_{ij} \\log(\\hat{y}_{ij}) $$\n",
    "  où \\(C\\) est le nombre de classes.\n",
    "\n",
    "- **Hinge Loss** pour les SVM :\n",
    "  $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\max(0, 1 - y_i \\cdot \\hat{y}_i) $$\n",
    "  - Utilisé principalement pour les modèles de marge comme les machines à vecteurs de support (SVM).\n",
    "\n",
    "- **Squared Hinge Loss** :\n",
    "  $$ J(\\theta) = \\frac{1}{m} \\sum_{i=1}^{m} \\max(0, 1 - y_i \\cdot \\hat{y}_i)^2 $$\n",
    "\n",
    "### 3. Fonctions de Coût pour des Cas Spécifiques\n",
    "\n",
    "- **Focal Loss** : Utilisée dans les problèmes de classification déséquilibrés pour mettre l'accent sur les exemples difficiles.\n",
    "  $$ J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\alpha (1 - \\hat{y}_i)^\\gamma y_i \\log(\\hat{y}_i) $$\n",
    "  où \\(\\alpha\\) et \\(\\gamma\\) sont des hyperparamètres.\n",
    "\n",
    "- **Kullback-Leibler Divergence** (KL Divergence) pour les modèles génératifs :\n",
    "  $$ J(\\theta) = \\sum_{i} p(x_i) \\log\\left(\\frac{p(x_i)}{q(x_i)}\\right) $$\n",
    "  - Mesure la différence entre deux distributions de probabilité.\n",
    "\n",
    "\n",
    "Ces fonctions de coût sont choisies en fonction du type de problème, de la nature des données et des objectifs spécifiques du modèle. Chaque fonction de coût a ses propres caractéristiques et est adaptée à des scénarios particuliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Une analogie avec le domaine d'étude\n",
    "Ces fonctions de coût sont adaptées aux besoins spécifiques de chaque domaine, permettant d'optimiser les modèles d'apprentissage automatique pour des applications pratiques dans la vie quotidienne. Le choix de la fonction de coût dépend souvent des particularités des données et des objectifs de chaque application.\n",
    "Voici un aperçu des fonctions de coût utilisées dans divers domaines de la vie quotidienne, illustrant comment l'apprentissage automatique est appliqué dans des situations réelles :\n",
    "\n",
    "## 1. Santé et Médecine\n",
    "\n",
    "- **Fonction de Coût** : **Erreur Quadratique Moyenne (MSE)**\n",
    "  - **Utilisation** : Estimation des paramètres dans les modèles prédictifs pour des maladies (par exemple, prédire les niveaux de glucose).\n",
    "  - **Référence** : \"Statistical Methods in Medical Research\" par Armitage et Berry.\n",
    "\n",
    "- **Fonction de Coût** : **Entropie Croisée**\n",
    "  - **Utilisation** : Classification des patients en fonction de divers diagnostics médicaux.\n",
    "  - **Référence** : \"Pattern Recognition and Machine Learning\" par Christopher Bishop.\n",
    "\n",
    "## 2. Finance et Économie\n",
    "\n",
    "- **Fonction de Coût** : **Erreur Absolue Moyenne (MAE)**\n",
    "  - **Utilisation** : Modèles de prévision des ventes ou des prix d'actions.\n",
    "  - **Référence** : \"Forecasting: Methods and Applications\" par Makridakis et Hibon.\n",
    "\n",
    "- **Fonction de Coût** : **Log-Likelihood**\n",
    "  - **Utilisation** : Modèles de risque de crédit pour évaluer la probabilité de défaut.\n",
    "  - **Référence** : \"Econometric Analysis\" par Greene.\n",
    "\n",
    "## 3. Marketing et Publicité\n",
    "\n",
    "- **Fonction de Coût** : **Entropie Croisée**\n",
    "  - **Utilisation** : Modèles de classification pour cibler les annonces en fonction des comportements des utilisateurs.\n",
    "  - **Référence** : \"Deep Learning\" par Ian Goodfellow et al.\n",
    "\n",
    "- **Fonction de Coût** : **Focal Loss**\n",
    "  - **Utilisation** : Amélioration de la détection des clients à faible engagement dans des campagnes publicitaires.\n",
    "  - **Référence** : \"Focal Loss for Dense Object Detection\" par Lin et al.\n",
    "\n",
    "## 4. Transport et Logistique\n",
    "\n",
    "- **Fonction de Coût** : **Erreur Quadratique Moyenne (MSE)**\n",
    "  - **Utilisation** : Estimation des temps de trajet et des coûts de livraison.\n",
    "  - **Référence** : \"Logistics and Supply Chain Management\" par Martin Christopher.\n",
    "\n",
    "- **Fonction de Coût** : **Hinge Loss**\n",
    "  - **Utilisation** : SVM pour la classification des itinéraires optimaux.\n",
    "  - **Référence** : \"The Elements of Statistical Learning\" par Hastie, Tibshirani et Friedman.\n",
    "\n",
    "## 5. Éducation\n",
    "\n",
    "- **Fonction de Coût** : **Entropie Croisée**\n",
    "  - **Utilisation** : Systèmes de recommandation d'apprentissage pour prédire les préférences des étudiants.\n",
    "  - **Référence** : \"Recommender Systems: The Textbook\" par Charu Aggarwal.\n",
    "\n",
    "- **Fonction de Coût** : **Mean Squared Error (MSE)**\n",
    "  - **Utilisation** : Modèles prédictifs pour évaluer les performances académiques.\n",
    "  - **Référence** : \"Educational Data Mining: A Review of the State of the Art\" par Romero et Ventura.\n",
    "\n",
    "## 6. Agriculture\n",
    "\n",
    "- **Fonction de Coût** : **Erreur Absolue Moyenne (MAE)**\n",
    "  - **Utilisation** : Prédiction des rendements des cultures en fonction des conditions météorologiques.\n",
    "  - **Référence** : \"Precision Agriculture for Sustainability\" par R. S. K. S. Kumar et al.\n",
    "\n",
    "- **Fonction de Coût** : **Kullback-Leibler Divergence**\n",
    "  - **Utilisation** : Modèles pour évaluer les variations dans les distributions de sol.\n",
    "  - **Référence** : \"Information Theory and Statistics\" par Solomon Kullback.\n",
    "\n",
    "## 7. Environnement\n",
    "\n",
    "- **Fonction de Coût** : **Mean Squared Error (MSE)**\n",
    "  - **Utilisation** : Modèles de prévision de la qualité de l'air ou des niveaux de pollution.\n",
    "  - **Référence** : \"Environmental Modelling with GIS and Remote Sensing\" par J. P. Wilson.\n",
    "\n",
    "- **Fonction de Coût** : **Log-Cosh**\n",
    "  - **Utilisation** : Modèles prédictifs pour les changements climatiques.\n",
    "  - **Référence** : \"Deep Learning for Time Series Forecasting\" par Jason Brownlee.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans un modèle de régression linéaire, l'équation générale est :\n",
    "\n",
    "$$\n",
    "\\hat{y} = w_0 + w_1x_1 + w_2x_2 + ... + w_nx_n\n",
    "$$\n",
    "\n",
    "où \\( w_0 \\) est le terme d'interception (biais). En ajoutant une colonne de 1, on permet au modèle d'apprendre cette valeur d'interception."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Fonction de Coût Recommandée\n",
    "\n",
    "**Entropie Croisée (Binary Cross-Entropy)**\n",
    "\n",
    "La fonction de coût recommandée pour la classification binaire est l'entropie croisée, formulée comme suit :\n",
    "\n",
    "$$\n",
    "J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^{m} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "### Justification\n",
    "\n",
    "1. **Nature Binaire** : Cette fonction est conçue spécifiquement pour des problèmes de classification binaire, où les classes sont souvent représentées par 0 et 1 (maligne vs bénigne).\n",
    "\n",
    "2. **Sensibilité aux Erreurs** : L'entropie croisée pénalise fortement les prédictions incorrectes, particulièrement lorsque le modèle est très confiant dans une mauvaise prédiction. Cela améliore l'apprentissage à partir des exemples difficiles.\n",
    "\n",
    "3. **Probabilités** : Elle traite les sorties du modèle comme des probabilités (valeurs comprises entre 0 et 1), ce qui est essentiel pour les modèles de classification, en particulier ceux utilisant des fonctions logistiques.\n",
    "\n",
    "4. **Convergence Rapide** : L'entropie croisée permet une convergence plus rapide lors de l'entraînement des modèles, car elle fournit des gradients significatifs même lorsque les prédictions sont proches des classes.\n",
    "\n",
    "5. **Facilité d'Interprétation** : Elle peut être interprétée comme une mesure de la surprise ou de l'incertitude dans les prédictions du modèle, ce qui est utile pour l'analyse des performances.\n",
    "\n",
    "### Propriétés de la Fonction de Coût\n",
    "\n",
    "1. **Non-négativité** : La fonction de coût est toujours positive ou nulle. Cela signifie qu'elle ne peut pas avoir de valeur négative, ce qui est logique car elle représente une mesure d'erreur.\n",
    "\n",
    "2. **Convexité** : L'entropie croisée est une fonction convexe par rapport aux paramètres du modèle. Cela garantit qu'il existe un minimum global, facilitant ainsi l'optimisation des paramètres.\n",
    "\n",
    "3. **Dérivabilité** : La fonction est dérivable, ce qui permet d'utiliser des méthodes d'optimisation basées sur le gradient, comme la descente de gradient, pour ajuster les paramètres du modèle.\n",
    "\n",
    "4. **Robustesse** : Elle est moins sensible aux valeurs aberrantes dans le cas où les probabilités prédites sont proches de 0 ou 1, ce qui la rend robuste par rapport à certaines erreurs de prédiction.\n",
    "\n",
    "5. **Interprétabilité** : Les valeurs de l'entropie croisée peuvent être interprétées dans un sens probabiliste, ce qui aide à comprendre l'incertitude associée aux prédictions du modèle.\n",
    "\n",
    "En résumé, pour la base de données **load_breast_cancer**, l'entropie croisée est la fonction de coût recommandée en raison de sa pertinence pour la classification binaire, de sa capacité à gérer efficacement les erreurs, de ses propriétés favorables (non-négativité, convexité, dérivabilité), et de sa robustesse. Ces caractéristiques en font un choix optimal pour l'entraînement de modèles de classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***verification et calcul de l'erreur de généralisation***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Proximal graddient descend**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Quasi-NEWTON**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Nesterov Accelerated Gradient(NAG)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **verification de l'erreur de généralisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ADAM**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **ADAGRAD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concept de Recuit Simulé\n",
    "\n",
    "Le recuit simulé est une méthode d'optimisation inspirée du processus de refroidissement des métaux. L'idée principale est d'explorer l'espace des solutions de manière à éviter de se coincer dans des minima locaux en acceptant occasionnellement des solutions moins bonnes. Cette approche est particulièrement utile dans les problèmes d'optimisation combinatoire où les solutions peuvent être nombreuses et complexes.\n",
    "\n",
    "### Formule Mathématique\n",
    "\n",
    "1. **Fonction de Coût** :\n",
    "   On cherche à minimiser une fonction de coût \\( C(w) \\), où \\( w \\) représente les poids du modèle. Dans le code, cette fonction est représentée par `logistic_loss(weights, X, y)`.\n",
    "\n",
    "2. **Changement de Coût** :\n",
    "   On calcule la différence de coût entre la nouvelle solution et la solution actuelle :\n",
    "   $$ \\Delta C = C(w_{\\text{new}}) - C(w_{\\text{current}}) $$\n",
    "\n",
    "3. **Critère d'Acceptation** :\n",
    "   La nouvelle solution est acceptée selon les règles suivantes :\n",
    "   - Si \\( \\Delta C < 0 \\) (c'est-à-dire que la nouvelle solution est meilleure), elle est acceptée.\n",
    "   - Sinon, elle est acceptée avec une probabilité donnée par :\n",
    "   $$ P(\\text{accept}) = \\exp\\left(-\\frac{\\Delta C}{T}\\right) $$\n",
    "   où \\( T \\) est la température actuelle. Cela permet d'accepter des solutions moins bonnes à haute température et d'être plus strict à basse température.\n",
    "\n",
    "4. **Refroidissement** :\n",
    "   La température diminue au fil des itérations selon la formule :\n",
    "   $$ T_{\\text{next}} = \\alpha \\cdot T_{\\text{current}} $$\n",
    "   où \\( \\alpha \\) est un facteur de refroidissement (généralement \\( 0 < \\alpha < 1 \\)).\n",
    "\n",
    "Cette méthode permet d'explorer efficacement l'espace des solutions tout en évitant de se bloquer dans des optima locaux, ce qui est crucial pour trouver des solutions optimales dans des problèmes complexes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
